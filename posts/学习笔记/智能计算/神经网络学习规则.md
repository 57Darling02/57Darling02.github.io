---
title: 神经网络学习规则
date: 2025-03-10 20:46:45
tags: 学习笔记
---
# 神经网络学习规则



## 一、学习规则对比表
| **规则名称**     | **公式**                                                     | **类型**   | **核心特点**                                     | **应用场景**                 |
| ---------------- | ------------------------------------------------------------ | ---------- | ------------------------------------------------ | ---------------------------- |
| **Hebb规则**     | $ \Delta w_{ij} = \eta \cdot o_j \cdot x_i $                 | 无监督学习 | 同时激活的神经元连接增强（"一起放电，一起连接"） | 自组织映射、特征提取         |
| **感知器规则**   | $ \Delta w_{ij} = \eta \cdot (d - y) \cdot x_i $             | 监督学习   | 基于误差修正，仅调整错误分类样本的权值           | 线性分类问题（如逻辑门实现） |
| **胜者为王规则** | $ \Delta w_{j^*} = \eta \cdot (X - w_{j^*}) $                | 无监督学习 | 竞争机制，仅调整获胜神经元的权值                 | 聚类分析、特征映射           |
| **δ学习规则**    | $ \Delta w_{ij} = \eta \cdot (d - y) \cdot f'(net) \cdot x_i $ | 监督学习   | 梯度下降优化，适用于非线性激活函数               | 多层感知器、BP算法           |
| **BP算法**       | $ \Delta w_{jk} = \eta \cdot \delta_k^o \cdot y_j $          | 监督学习   | 误差反向传播，支持多层网络训练                   | 复杂模式识别（如异或问题）   |

> 更多例题：https://blog.csdn.net/qq_17517409/article/details/106027837

### 赫布法则

**两个神经细胞交流越多，它们连接的效率就越高，反之就越低。**赫布希望人工神经这样学习。**学习率x输出x输入**

## 一、题目描述

设有4输入单输出神经元网络，参数如下：
- **阈值**：$ T = 0 $
- **学习率**：$ \eta = 1 $
- **输入样本**：
  $
  X^1 = (1, -2, 1.5, 0)^T, \quad X^2 = (1, -0.5, -2, -1.5)^T, \quad X^3 = (0, 1, -1, 1.5)^T
  $
- **初始权向量**：
  $
  W(0) = (1, -1, 0, 0.5)^T
  $
- **激活函数**：双极性符号函数 $ f(\text{net}) = \text{sgn}(\text{net}) $

## 二、解答步骤

### 1. 输入第一个样本 $ X^1 $

#### （1）计算净输入

$
\text{net}^1 = W(0)^T X^1 = (1, -1, 0, 0.5) \cdot (1, -2, 1.5, 0)^T = 3 \quad 
$

#### （2）调整权向量

$
\begin{aligned}
W(1) &= W(0) + \eta \cdot \text{sgn}(\text{net}^1) \cdot X^1 \\
&= (1, -1, 0, 0.5)^T + 1 \cdot 1 \cdot (1, -2, 1.5, 0)^T \\
&= (2, -3, 1.5, 0.5)^T \quad 
\end{aligned}
$


### 2. **输入第二个样本 $ X^2 $**
#### （1）计算净输入

$
\text{net}^2 = W(1)^T X^2 = (2, -3, 1.5, 0.5) \cdot (1, -0.5, -2, -1.5)^T = -0.25 \quad 
$

#### （2）调整权向量
$
\begin{aligned}
W(2) &= W(1) + \eta \cdot \text{sgn}(\text{net}^2) \cdot X^2 \\
&= (2, -3, 1.5, 0.5)^T + 1 \cdot (-1) \cdot (1, -0.5, -2, -1.5)^T \\
&= (1, -2.5, 3.5, 2)^T \quad 
\end{aligned}
$

### 3. **输入第三个样本 $ X^3 $**

#### （1）计算净输入

$
\text{net}^3 = W(2)^T X^3 = (1, -2.5, 3.5, 2) \cdot (0, 1, -1, 1.5)^T = -3 \quad 
$

#### （2）调整权向量
$
\begin{aligned}
W(3) &= W(2) + \eta \cdot \text{sgn}(\text{net}^3) \cdot X^3 \\
&= (1, -2.5, 3.5, 2)^T + 1 \cdot (-1) \cdot (0, 1, -1, 1.5)^T \\
&= (1, -3.5, 4.5, 0.5)^T \quad 
\end{aligned}
$



## 离散感知器学习规则

​	散感知器学习规则则代表一种**有导师的学习方式**，**学习率x误差x输入**

