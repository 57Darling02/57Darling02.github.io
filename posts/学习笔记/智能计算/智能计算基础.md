---
title: 智能计算基础
date: 2025-05-26 13:46:55
tags: 学习笔记 智能计算
---
# 智能计算基础

> 临时抱佛脚🥲

## 第二章 神经网络基础知识

### 人的神经网络结构

![image-20250526135142628](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526135142628.png)

- 细胞体(Cell body)
- 树突(Dendrite)
	- 这些突起称为树突;神经元靠树突接受来自其他神经元的输入信号，相当于细胞体的**输入端**
- 轴突(Axon)
	- 突触(Synapse)：轴突的末梢，相当于神经元之间的输入/输出接口
	- 用来传出细胞体产生的输出电化学信号，相当于细胞体的**输出端**

**注：**

- **多输入端**，每个神经元可以有一或多个树突。
- **单输出**，每个神经元只有一个轴突，可以把兴奋从胞体传送到另一个神经元或其他组织。
- 一个神经元接受的信息，在时间和空间上常呈现出一种复杂多变的形式，
	需要神经元对它们进行**积累和整合加工**，从而**决定其输出的时机和强度**。正是
	神经元这种整合作用，才使得亿万个神经元在神经系统中有条不紊、夜以继日
	地处理各种复杂的信息，**执行着生物中枢神经系统的各种信息处理功能**。

### 人工神经元的建模

M-P模型模型的六点假设:

- 每个神经元都是一个**多输入单输出**的信息处理单元
- 神经元输入分**兴奋性输入和抑制性输入**两种类型
- 神经元**有空间整合特性和闽值特性**
- 神经元输入与输出间有固定的时滞,主要取决于突触延搁;
- 忽略时间整合作用和**不应期**
- 神经元本身是**非时变**的，即其突触时延和突触强均为常数。

![image-20250526140519741](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526140519741.png)

#### 多输入单输出数学模型

![image-20250526140741459](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526140741459.png)

#### M-P模型假设

1. 多输入单输出
2. 输入分兴奋性/抑制性
3. 空间整合与阈值特性
4. 固定突触时延
5. 忽略不应期及时变特性

#### （2）数学表达式

$$
y(t) = f\left( \sum_{i=1}^n w_i x_i(t-\tau_i) - \theta \right)
$$

- $w_i$：输入权重
- $\theta$：阈值
- $f$：激活函数

#### （3）神经元的转移函数(激活函数)类型

| 类型              | 公式                                                         | 特点                   |
| ----------------- | ------------------------------------------------------------ | ---------------------- |
| 阈值型信号函数sgnal | $f(x) = \begin{cases} 1 & x \geq 0 \\ 0 & x < 0 \end{cases}$ | 二值输出               |
| 非线性转移Sigmoid | $f(x) = \frac{1}{1+e^{-x}}$                                  | 平滑非线性，输出[0,1]  |
| 分段线性ReLU      | $f(x) = \max(0, x)$                                          | 缓解梯度消失，计算高效 |
| LeakyReLU         | $f(x) = \max(0, x) + \gamma \min(0, x)$                      | 避免死亡ReLU问题       |
| 概率型转移函数    |$f(x) = \frac{1}{1+e^{-x/T}}$                                       |                   |

非线性转移Sigmoid特性：

![img](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/06abbe5c050770300de9c91bc76e4af5.svg)

#### 人工神经网络模型

| 类型       | 结构特点             | 典型应用     |
| ---------- | -------------------- | ------------ |
| 层次型     | 分层连接，无反馈     | 前馈神经网络 |
|            | 输出层到输入层有连接 | 反馈神经网络 |
| 层内互连型 | 层内有连接           |              |
| 局部互连型 | 节点仅与邻近节点连接 | 卷积神经网络 |
| 全互连型   | 任意节点间直接连接   | Hopfield网络 |

1. 拓扑型
	- 单纯层次
	- 输出层到输入层有连接
	- 层内有连接
	- 全互连
	- 局部互连
2. 信息流向型
	- 前馈型网络
	- 反馈型网络

### 神经网络的学习

#### 内涵

- **学习方法是体现人工神经网络智能特征的主要指标**
- 所谓训练，就是在将由样本向量构成的样本集合输入到人工神经网路的过程中，**按照一定方式去调整神经元之间的连接权**。使网络能将样本集的内涵以连接权矩阵的方式存储起来，从而使网络在数据输入时可以给出适当的输出。

#### 分类

- 监督学习(SupervisedLearning，SL)

	> 通过给定的训练数据集中“学习”出一个函数，当新的数据到来时，可以根据这个函数预测结果。
	>
	> 主要用途：分类（Classify）和回归（Regression）

- 非监督学习(Unsupervised learning,，URL).

	> 只提供事物的具体特征(特征值)，但不提供事物的名称(目标值)，让学习者自己总结归纳。是将数据集合分成由类似的对象组成的多个簇（或组）的过程。
	>
	> 主要用途：聚类和降维问题

- 强化学习(Reinforcement Learning,

	> 是系统从环境到行为映射的学习，以使激励信号（强化信号）函数值最大化。
	>
	> 强化学习特点包括：
	>
	> ①学习过程中没有监督者，只有激励信号；
	>
	> ②反馈信号是延迟而非即时的；
	>
	> ③学习过程具有时间序列性质；
	>
	> ④系统的动作会影响到后续的数据。

![image-20250526142251188](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526142251188.png)



####  Hebb学习规则

> 基于“当神经元i与神经元j同时处于兴奋状态时，两者的连接强度应增加”。两个相连的神经元，理解为神经元的输入（为与之相连的神经元的输出）与输出同时为正

**输入输出同时为正时加强**

注意：

* Hebb学习规则是一种纯前馈、无导师学习
* Hebb学习规则需要预先设置权饱和值，以防止输入和输出正负始终一致时出现权值无约束增长

由于不考计算，看到个大概得了。

![image-20250526184224380](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526184224380.png)



## 第三章 感知器

### 单层感知器

#### 结构

![image-20250526184543763](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526184543763.png)

#### 模型

![image-20250526184630928](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526184630928.png)

#### 功能

##### 二维平面分类

![image-20250526184714368](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526184714368.png)



##### 三维平面分类

![image-20250526184840770](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526184840770.png)



##### 总结

分类功能

- 一个最简单的单计算节点感知器具有分类功能。
- 分类原理：将分类知识存储于感知器的权向量（包含了阈值）中，由权向量确定的分类判决界面将输入模式分为两类。

#### 学习算法步骤

1. 初始化权值和阈值（小随机数）
2. 输入样本计算净输入$net = W^T X$
3. 计算输出$y = \text{sgn}(net)$
4. 调整权值：$W(t+1) = W(t) + \eta (d - y) X$，其中$d$为期望输出
5. 重复直到所有样本分类正确



**单层感知器无法处理的非线性分类问题（如异或问题）**

### 多层感知器

解决单层感知器无法处理的非线性分类问题（如异或问题）。

#### （1）结构特点

- 包含一个或多个隐层
- 隐层使用非线性激活函数（如Sigmoid）
- 输入层→隐层→输出层的层次结构

#### （2）分类能力对比

| 层数   | 判决域形状   | 典型问题       |
| ------ | ------------ | -------------- |
| 单层   | 超平面       | 线性可分问题   |
| 单隐层 | 凸区域       | 非线性可分问题 |
| 双隐层 | 任意复杂形状 | 复杂模式识别   |

通过引入隐层和非线性激活函数，多层感知器能够逼近任意非线性函数。



### BP算法

> 利用输出层的误差来估计输出层的直接前导层的误差，再用这个误差估计更前一层的误差，如此一层一层的反传下去，就获得了所有其他各层的误差估计。

属于**有监督学习**

#### 核心思想

将输出误差以某种形式通过隐层向输入层逐层反传，将**误差分摊**给各层的所有单元；各层单元的误差信号修正各单元权值。

#### 基于BP算法的多层前馈网络模型

1. **误差定义**：
	$E = \frac{1}{2} \sum_{k=1}^m (d_k - o_k)^2$

2. **权值调整公式**：
	- 输出层：
		$\Delta w_{jk} = \eta \delta_k^o y_j$
		其中$\delta_k^o = (d_k - o_k) o_k (1 - o_k)$
	- 隐层：
		$\Delta v_{ij} = \eta \delta_j^y x_i$
		其中$\delta_j^y = y_j (1 - y_j) \sum_{k} \delta_k^o w_{jk}$



#### 算法步骤

1. 初始化权值（小随机数）
2. 正向传播计算输出
3. 计算输出层误差$\delta^o$
4. 反向传播计算隐层误差$\delta^y$
5. 更新权值
6. 重复直到误差收敛精度达标

#### 相关概念

1. Epoch 全部数据训练一次称为 1个epoch；
    epochs：指训练过程中数据将被“轮”多少次（迭代次数）
2. Batch 如果据量太大，将数据分成几块，就是分成几个batch。
3. batch size一个batch中的数据量大小即为batch size。
4. iterations（批迭代）：每一次迭代都是一次权重更新，每一次
    权重更新需要batch_size个数据进行Forward运算得到损失函数，
    在BP算法更新参数

#### 误差曲面与BP局限

![image-20250526200256382](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526200256382.png)

##### 误差曲面的分布有两个特点：

- 存在平坦区域 
	- 原因：各节点的净输入过大
- 存在多个极小点
	- 多数极小点都是局部极小，即使是全局极小往往也不是唯一的，但其特点都是误差梯度为零。极大影响收敛速度

##### 缺陷

- 易形成局部极小而得不到全局最优；
- 训练次数多，使得学习效率低，收敛速度慢；
- 隐节点的选取缺乏理论指导；
- 训练时学习新样本有遗忘旧样本的趋势。

##### 改进

- 附加动量的改进算法
	- 在反向传播法的基础上，在每一个权值（或阈值）的变化上加上一项正比于上一次权值（或阈值）变化量的值，并根据反向传播法来产生新的权值（或阈值）变化。
- 自适应调整参数的改进算法
	- 采用自适应调整参数的改进算法的基本设想是学习率应根据误差变化而自适应调整，以使权系数调整向误差减小的方向变化

#### 多层前馈网(感知器)的主要能力

- 非线性映射能力

​	多层前馈网能学习和存贮大量输入-输出模式映射关系。

- 泛化能力

​	当向网络输入训练时未曾见过的非样本数据时，网络也能完成由输入空间向输出空间的正确映射。

- 容错能力

​	输入样本中带有较大的误差甚至个别错误对网络的输入输出规律影响很小。

### 神经网络学习处理过程

#### 训练样本集的准备

- 样本一定要有代表性

- 样本类别的均衡，尽量使每个类别的样本数量大致相等。
- 样本的组织要注意将不同类别的样本交叉输入，或从训练集中随机选择输入样本

#### 输入输出数据的归一化

归一化也称为或标准化，是指通过变换处理将网络的输入、输出数据限制在[0，1]或[-1，1]区间内。
##### 进行归一化的主要原因

①网络的各个输入数据常常具有不同的物理意义和不同的量纲，归一化给各输入分量以同等重要的地位;

②归一化后可防止因净输入的绝对值过大而使神经元输出饱和，继而使权值调整进入误差曲面的平坦区;

③教师信号如不进行归一化处理，势必使数值大的输出分量绝对误差大，数值小的输出分量绝对误差小。

#### 网络训练与测试

隐节点数一定的情况下，为获得好的泛化能力，存在着一个最佳训练次数。



### BP网络结构设计

#### 初始权值的设计

目标:为了使各节点的初始净输入在零点附近

- 使初始权值足够小，主要用于隐层权值
- 使初始值为+1和-1的权值数相等，主要用于输出层权值

​	因为从隐层权值调整公式来看，如果输出层权值太小会使隐层权值在训练初期的调整量变小。因此采用了第二种权值与净输入兼顾的办法。

#### 隐层数的设计

​	理论分析证明，具有**单隐层的前馈网可以映射所有连续函数**，只有当学习不连续函数(如锯齿波等)时，才需要两个隐层，所以多层前馈网最多只需两个隐层。

​	经验表明，采用两个隐层时，如在第一个隐层设置较多的隐节点而第二个隐层设置较少的隐节点，则有利于改善多层前馈网的性能。

## 第四章

### 竞争学习的概念与原理

- **基本概念**
	- **模式相关概念**：模式是事物的标准样式等，模式类是具有共同特征的模式集合。分类是在导师信号指导下将输入模式分配到模式类，**无导师指导的分类即聚类** ，依据相似性将相似模式样本归为一类。
	- **相似性测量**
		- **欧式距离法**：两个模式向量欧式距离越小越相似，可设定最大欧式距离T作为聚类判据。公式为$\left\| X - X_{i}\right\| =\sqrt{(X - X_{i})^{T}(X - X_{i})}$。
		- **余弦法**：模式向量夹角越小，余弦越大越相似，设置最大夹角$\Psi_{T}$作为聚类判据，适用于向量长度相同或特征与方向相关的测量。公式为$cos \psi=\frac{X^{T} X_{i}}{\| X\| \left\| X_{i}\right\| }$。
		- **内积法**：内积值越大相似度越高，当模式方向相同且长度相等时取最大值。公式为$X^{T} X_{i}=\| X\| \left\| X_{i}\right\| cos \psi$。
	- **侧抑制与竞争**：人眼等存在侧抑制现象，神经细胞兴奋会抑制周围细胞，最强抑制作用下竞争获胜者“惟我独兴”，即胜者为王。
	- **向量归一化**：将向量变成方向不变长度为1的单位向量，便于比较向量夹角，公式为$\hat{X}=\frac{X}{\| X\| }=\left(\frac{x_{1}}{\sqrt{\sum_{j = 1}^{n} x_{j}^{2}}}, \cdots, \frac{x_{n}}{\sqrt{\sum_{j = 1}^{n} x_{j}^{2}}}\right)^{T}$。
- **竞争学习原理**
	- **竞争学习规则（胜者为王算法）**
		- **向量归一化**：将当前输入模式向量X和竞争层各神经元权向量$W_{j}$归一化，得到$\hat{X}$和$\widehat{W}_{j}$。
		- **寻找获胜神经元**：通过计算欧式距离或夹角余弦，找出与$\hat{X}$最相似的权向量$\widehat{W_{j^{*}}}$作为获胜神经元，$\left\| \hat{X}-\hat{W}_{j^{*}}\right\| =min _{j \in\{1,2, \cdots, m\}}\left\{\left\| \hat{X}-\hat{W}_{j}\right\| \right\}$，也可转化为求最大点积$\hat{W}_{j^{*}}^{T} \hat{X}=max _{j \in\{1,2, \cdots, m\}}\left(\hat{W}_{j}^{T} \hat{X}\right)$。
		- **网络输出与权值调整**：获胜神经元输出为1，其余为0。获胜神经元权向量调整公式为$W_{j^{*}}(t + 1)=\hat{W}_{j^{*}}(t)+\mu(t)(\hat{X}-\hat{W}_{j^{*}})$，调整后需重新归一化，直到学习率$\mu(t)$衰减到0。
	- **竞争学习原理示例**：以将模式分为2类为例，通过随机初始化权向量，按算法调整权值，经过多次训练后，权向量逐渐接近模式类中心向量。

### 自组织特征映射（SOM）神经网络

- **SOM网络的生物学基础**：人脑感觉通道神经元有序排列，外界特定时空信息使大脑皮层特定区域兴奋，类似信息在对应区域连续映像，这是SOM网络竞争机制的生物学基础。训练后，SOM网络竞争层神经元会形成有序排列，功能相近的神经元靠近。
- **SOM网的拓扑结构与权值调整域**
	- **拓扑结构**：SOM网有输入层和输出层（竞争层），输出层神经元排列形式包括一维线阵、二维平面阵和三维栅格阵等。
	- **权值调整域**：SOM网学习算法（Kohonen算法）在胜者为王算法基础上改进，获胜神经元及其邻近神经元都调整权向量。调整方式可用墨西哥帽函数、大礼帽函数和厨师帽函数表示，常使用简化函数。以获胜神经元为中心设定优胜邻域，邻域半径随训练次数增加而收缩。
- **自组织特征映射网的运行原理与学习算法**
	- **运行原理**：分为训练和工作阶段。训练阶段，输入样本使输出层有节点获胜，获胜节点及其优胜邻域内节点权向量向输入向量方向调整，最终输出层各节点对特定模式类敏感，形成反映样本模式类分布的有序特征图。
	- **学习算法（Kohonen算法）**
		- **初始化**：对输出层权向量赋小随机数并归一化，建立初始优胜邻域，设定学习率初始值。
		- **接受输入**：从训练集中随机选取并归一化输入模式。
		- **寻找获胜节点**：计算输入模式与权向量点积，找出点积最大的获胜节点；若输入未归一化，则计算欧式距离找最小距离节点。
		- **定义优胜邻域**：以获胜节点为中心确定权值调整域，邻域随训练时间收缩。
		- **调整权值**：对优胜邻域内节点按$w_{i j}(t + 1)=w_{i j}(t)+\eta(t, N)[x_{i}^{p}-w_{i j}(t)]$调整权值，$\eta(t, N)$与训练时间和拓扑距离有关，如$\eta(t, N)=\eta(t) e^{-N}$ ，$\eta(t)$是单调下降函数。
		- **结束检查**：当学习率衰减到零或达到预定训练次数时结束训练，否则返回接受输入步骤。
	- **特征**
		- **保序映射**：能将输入空间样本模式类**有序映射**在输出层，如将不同动物按属性特征映射到二维输出平面，属性相似的动物位置相近。
		- **数据压缩**：可将高维空间样本在**保持拓扑结构不变**的条件下投影到低维空间，如29维动物属性向量经SOM网压缩为二维平面数据。
		- **特征抽取**：从**高维空间样本向低维空间映射**，可发现数据内在规律，如字符排序中根据向量分量相同情况分类，SOM网输出结果与树形结构相似。

### 自组织特征映射网络的设计与应用

- **SOM网的设计基础**
	- **输出层设计**：节点数设计要合适，过少无法区分全部模式类，过多可能分类过细或出现死节点。节点排列形式依实际应用选择，如旅行路径用二维平面、一般分类用一维线阵、机器人手臂控制用三维栅格。
	- **权值初始化问题**：权值一般初始化为较小随机数，可从训练集中随机抽取样本作为初始权值，或计算全体样本中心向量后迭加小随机数作为初始值。
	- **优胜邻域$N_{j^{*}}(t)$的设计**：邻域不断缩小，形状有正方形、六边形或圆形等，大小用邻域半径表示，可按经验选择，如$r(t)=C_{1}\left(1-\frac{t}{t_{m}}\right)$或$r(t)=C_{1} e^{-B_{1} t / t_{m}}$ 。
	- **学习率η(t)的设计**：训练开始时学习率可较大，快速下降捕捉输入向量大致结构，然后在较小值缓降至0，精细调整权值，如$\eta(t)=C_{2}\left(1-\frac{t}{t_{m}}\right)$或$\eta(t)=C_{2} e^{-B_{2} t / t_{m}}$ 。
- **应用与设计实例**：给定输入模式设计SOM网，如设置学习率、优胜邻域半径变化，观察训练过程中权向量变化，最终得到输入模式在输出平面的映射图。 



## 第六章 Hopfield神经网络

hopefield网络特征 作用 结构（联想，补全，记忆）

![image-20250526211500358](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250526211500358.png)



Hopfield网络是一种基于反馈机制的非线性动力学系统，核心特征是通过神经元间的相互连接形成动态记忆存储与联想能力。以下从**本质特征、核心作用、实现机制、结构原理**四方面概括总结：

#### 本质特征：反馈动力学与能量驱动的记忆系统

1. **反馈连接的非线性网络**  
   - 由神经元全连接构成单层反馈网络（无隐藏层），每个神经元输出通过权值反馈至其他神经元，形成动态闭环。  
   - 权值矩阵**对称且无自反馈**（$w_{ij}=w_{ji}, w_{ii}=0$），确保网络稳定性。  

2. **能量函数主导的收敛机制**  
   - 定义能量函数$E = -\frac{1}{2}X^TWX + X^TT$，网络状态更新遵循**能量最小化原则**（异步更新时能量单调递减）。  
   - 稳定状态（吸引子）对应能量函数的局部极小值，网络从初态向吸引子的收敛过程本质是**能量驱动的优化过程**。  

3. **离散与连续双模型**  
   - **离散型（DHNN）**：神经元状态取离散值（如$\{1, -1\}$或$\{1, 0\}$），适用于模式记忆与联想。  
   - **连续型（CHNN）**：状态为连续值，通过微分方程描述动态过程，适用于优化计算（如TSP问题）。

#### **二、核心作用：联想、补全与记忆的本质**

| **作用**     | **本质机制**                                           | **实现逻辑**                                                 |
| ------------ | ------------------------------------------------------ | ------------------------------------------------------------ |
| **联想记忆** | 从部分/含噪信息恢复完整记忆模式，基于吸引域的“吸引力”  | 初态作为部分信息落入某吸引子的吸引域，网络通过状态更新收敛至对应吸引子（完整模式）。 |
| **模式补全** | 对不完整输入填充缺失信息，本质是联想记忆的逆向过程     | 输入含缺失的不完整模式，网络动态演化至最接近的吸引子，自动补全缺失维度。 |
| **记忆存储** | 将模式存储为网络的稳定吸引子，通过权值矩阵编码记忆信息 | 利用Hebb规则等方法将模式编码为权值矩阵，吸引子数量受网络容量限制（理论上限为神经元数$n$）。 |

#### **三、实现方式：权值设计与动态演化**

1. **权值设计方法**  
   - **外积和法（Hebb规则）**：适用于正交模式存储，权值矩阵为模式外积之和$w_{ij} = \sum_{p=1}^{P}x_i^p x_j^p$（$i≠j$），确保模式为吸引子。  
   - **联立方程法**：通过不等式组约束权值与阈值，手动设计少量吸引子（如$net_j ≥0$对应状态$1$）。  

2. **动态演化过程**  
   - **异步更新**：每次仅更新一个神经元，能量严格递减，确保收敛至吸引子（定理6.1）。  
   - **同步更新**：所有神经元同时更新，需权矩阵为非负定对称阵以保证收敛（定理6.2）。  

3. **吸引子与吸引域**  
   - **吸引子**：满足$X = f(WX - T)$的稳定状态，可视为记忆的“存储地址”。  
   - **吸引域**：能收敛至同一吸引子的初态集合，吸引域越大，联想容错性越强。

#### **四、结构原理：全连接反馈的物理意义**

1. **拓扑结构**  
   - **单层全连接**：$n$个神经元两两互连（无自环），形成无向图结构，权值矩阵对称（$W = W^T$）。  
   - **阈值调节**：每个神经元设阈值$T_j$，调整激活难度（相当于偏置项）。  

2. **信息存储本质**  
   - 权值矩阵$W$是记忆的物理载体，每个元素$w_{ij}$编码神经元$i$与$j$的关联强度。  
   - 存储模式通过赫布可塑性规则（Hebbian plasticity）“刻写”到权值中，即“同时激活的神经元间连接加强”。  

3. **动力学本质**  
   - Hopfield网络本质是**离散非线性动力系统**，状态更新轨迹由权值矩阵与能量函数决定。  
   - 联想过程等价于系统从初态向吸引子的**非线性动力学收敛**，无需迭代计算，仅通过反馈自动完成。

#### **总结：Hopfield网络的核心贡献**

​	Hopfield网络首次将**能量函数与反馈动力学**引入神经网络，实现了记忆存储、联想推理与优化计算的统一框架。其本质是通过**权值对称的全连接结构**和**能量最小化原理**，将抽象的记忆模式映射为物理可实现的动态系统稳定态，为神经计算、组合优化等领域奠定了基础。尽管存在存储容量限制和伪吸引子问题，但其“通过动力学过程实现智能”的思想深刻影响了后续神经网络发展（如玻尔兹曼机、深度信念网络）。

## 第七章 径向基函数网络（RBF网络）


主要讲了**径向基函数网络（RBF网络）**的原理和应用，核心内容可以用“**怎么用曲线拟合解决实际问题**”来串联，以下是通俗易懂的解读：

### **一、RBF网络是干啥的？——解决曲线拟合和分类问题**

- **核心思想**：  
  就像用尺子画直线、用曲线板画曲线一样，RBF网络是一种用数学函数（径向基函数）来**拟合复杂数据规律**的工具。比如，给定一堆散点数据（如气温随时间变化），它能找到一条最合适的曲线或曲面，把这些点连起来，同时预测新数据的位置。

- **应用场景**：  
  - **拟合/回归**：预测房价、股票走势等连续值问题。  
  - **分类**：区分垃圾邮件（如用“关键词距离”判断邮件类别）、图像识别等。

### **二、RBF网络的结构——像搭积木一样分层干活**

- **输入层**：接收原始数据（如房价的面积、楼层等特征）。  
- **隐藏层**（关键）：  
  - 每个隐藏层神经元对应一个“**径向基函数**”（如高斯函数），用来计算输入数据与某个“中心”的距离。  
  - **核心逻辑**：越靠近中心的数据，函数输出值越高（类似“物以类聚”，距离越近越相似）。  
- **输出层**：将隐藏层的结果加权求和，得到最终结果（如房价的具体数值）。


### **三、关键概念：径向基函数与插值问题**
- **径向基函数是什么？**  
  简单说就是“**距离决定输出**”的函数。比如：  
  - **高斯函数**：$g(x) = e^{-\frac{(x - c)^2}{\sigma^2}}$，其中$c$是中心，$\sigma$控制“胖瘦”。离中心$c$越近，函数值越接近1；越远则越接近0。  
  - **直观理解**：像投石入水，涟漪的大小取决于离石头落点（中心）的距离。

- **插值问题**：  
  比如给定5个点，用多项式拟合可能需要4次曲线，但多项式次数高了容易过拟合（把噪声当规律）。RBF网络用多个高斯函数叠加，每个函数对应一个数据点（或聚类中心），拟合更灵活，泛化能力更强。

### **四、正则化理论：避免过拟合的“紧箍咒”**

- **过拟合问题**：  
  如果模型太复杂（如隐藏层神经元太多），会把数据中的噪声也学进去，导致模型在新数据上表现差（好比背题不理解原理，换道题就不会做）。  
- **正则化怎么做？**  
  - 在目标函数中加入“**平滑项**”（正则项），强制要求拟合曲线不能太“颠簸”，平衡拟合精度和光滑度。  
  - 类比：考试答题时，既要答案正确（拟合误差小），又要步骤清晰（曲线平滑），才能得高分。


### **五、权值与中心的确定方法——模型训练的关键**
1. **权值确定（正则网络）**：  
   - 当隐藏层神经元数量等于样本数时，权值可通过矩阵运算直接求解（$W = (G + \lambda I)^{-1}d$），无需迭代，一步到位。  
   - 当神经元较少时，用最小二乘法或梯度下降法调整权值，类似BP网络的学习过程。

2. **中心确定方法**：  
   - **随机选择**：从样本中随机挑几个作为中心，简单但可能不准。  
   - **自组织选择（K-means聚类）**：  
     把相似样本聚成一类，每类的“中心点”作为径向基函数的中心（类似班级分组，选每组的组长作为代表）。  
     - 步骤：随机选初始中心→计算样本到中心的距离→归类→更新中心→重复直到稳定。


### **六、RBF与多层感知机（MLP）的对比——各有优劣的工具**
| **对比点**       | **RBF网络**                              | **MLP（如BP网络）**                      |
|------------------|------------------------------------------|-----------------------------------------|
| **隐藏层作用**    | 用径向基函数计算距离，提取局部特征       | 用激活函数进行非线性变换，提取全局特征  |
| **学习方式**      | 隐藏层中心通过聚类确定，权值线性求解，训练快 | 全网络参数迭代优化，可能陷入局部最优，训练慢 |
| **分类边界**      | 用超球面划分（如区分圆形区域内外的点）   | 用超平面划分（如直线/平面分割数据）     |
| **适用场景**      | 数据分布有明显局部特征，实时性要求高     | 复杂非线性问题，需要深度特征提取        |

### **七、总结：RBF网络的优势与本质**

- **优势**：  
  - 训练速度快（尤其适合数据量大的场景）。  
  - 避免过拟合能力强（正则化+局部响应）。  
  - 可解释性较高（中心和权值物理意义明确）。  
- **本质**：  
  通过“**距离度量+线性组合**”将复杂非线性问题转化为简单线性问题，就像用多个“局部探测器”（径向基函数）感知数据，再综合判断结果。适合解决需要快速建模和局部特征敏感的问题（如传感器数据拟合、实时控制系统）。



## 第八章 SVM（重要）

通俗易懂的语言对支持向量机（SVM）的核心概念和原理的讲解，避开复杂公式推导，聚焦本质理解：


### **一、SVM的核心目标：找一条“最安全”的分界线**
#### **1. 线性可分场景：大间隔分类器**
- **核心思想**：  
  假设你要把两类数据（比如猫和狗的图片）用一条直线（二维）或超平面（高维）分开，SVM的目标不是随便找一条分界线，而是找一条**离两边数据都最远的线**，这条线被称为“最大间隔超平面”。  
  
  - **类比**：在拥挤的地铁车厢里，想在人群中划出一条通道，让两边的人尽量远离通道，这样新上车的人更容易判断自己该站哪边。  
  - **关键术语**：  
    - **支持向量**：离分界线最近的那些数据点，它们决定了分界线的位置（就像划定通道时，最靠近通道的几个人的位置决定了通道的宽窄）。  
    - **间隔（Margin）**：分界线到两边支持向量的距离之和，间隔越大，分类越“安全”，泛化能力越强（不容易被新数据误导）。.
    
    

#### **2. 数学本质：最大化间隔的优化问题**

- **目标**：  
  找到一个超平面$w^T x + b = 0$，使得两类数据到超平面的最小距离（几何间隔）最大。  
  - **直观理解**：间隔越大，模型对未知数据的容错性越强，就像宽宽的马路不容易堵车，窄路容易拥堵（过拟合）。

### **二、处理非线性数据：核函数的“魔法”**

#### **1. 非线性问题的困境**  
  如果数据在原始空间中不是线性可分的（比如异或问题、环形分布的数据），直接用直线/超平面无法分开。这时候需要将数据从低维空间“映射”到高维空间，使其在高维空间中线性可分。  
  - **类比**：二维平面上的“环形数据”无法用直线分开，但把它“拎”到三维空间后，可能变成一个“圆盘”，用一个平面就能切开。

#### **2. 核函数：无需显式映射的“捷径”**

  - **核函数的作用**：  
    直接计算低维空间中数据点在高维空间的内积，避免了显式进行高维映射（否则会导致“维度爆炸”，计算量激增）。  
    - **数学本质**：假设低维空间的两个点$x_i, x_j$映射到高维空间后为$\phi(x_i), \phi(x_j)$，核函数$K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$，直接计算内积而无需知道$\phi$的具体形式。  
  - **类比**：你想知道两个物体在镜子中的距离，但不需要真的看到镜子里的图像，只需通过镜子的反射规律（核函数）就能算出结果。

#### **3. 常见核函数类型与适用场景**  
| **核函数**        | **特点**                                                     | **适用场景**                           |
| ----------------- | ------------------------------------------------------------ | -------------------------------------- |
| **线性核**        | 直接计算内积$K(x_i, x_j) = x_i \cdot x_j$，无映射，简单高效。 | 线性可分数据（如文本分类初步筛选）。   |
| **多项式核**      | $K(x_i, x_j) = (x_i \cdot x_j + c)^d$，引入多项式特征，提升非线性能力。 | 图像边缘检测、简单非线性分类。         |
| **高斯核（RBF）** | $K(x_i, x_j) = e^{-\frac{\|x_i - x_j\|^2}{2\sigma^2}}$，局部敏感，灵活。 | 通用非线性问题（如手写识别、生物数据） |
| **Sigmoid核**     | $K(x_i, x_j) = \tanh(\beta x_i \cdot x_j + c)$，类似神经网络激活函数。 | 特定结构的非线性问题（较少用）。       |

  - **关键参数**：  
    - 高斯核的$\sigma$：控制数据点的“影响力范围”，$\sigma$越小，模型越复杂（容易过拟合）；$\sigma$越大，模型越简单（可能欠拟合）。  
    - 多项式核的$d$：次数越高，模型越复杂，但容易过拟合。


### **三、处理噪声与异常值：松弛变量与软间隔**
#### **1. 现实数据的不完美**  
  实际数据中可能存在噪声或异常点（比如猫的图片里混了一张狗的图片标错了标签），强行用硬间隔（必须完全分开）会导致模型过拟合。  
  - **解决方法**：引入**松弛变量$\xi_i$**，允许某些样本暂时“越界”（错误分类），但通过参数$C$控制惩罚力度。  
    - **类比**：马路中间的护栏允许偶尔有行人翻越（噪声样本），但翻越的人越多（$\xi_i$越大），罚款越重（$C$越大），需要在“护栏严格性”（间隔大小）和“行人自由度”（分类误差）之间权衡。

#### **2. 软间隔的直观理解**  
  - **参数$C$的作用**：  
    - $C$越大，对错误分类的惩罚越严格，模型倾向于减少错误，但可能过拟合（护栏太严，连正常行人也被误判为翻越）。  
    - $C$越小，允许更多错误，模型更关注间隔最大化，泛化能力强，但可能欠拟合（护栏太松，分不清行人方向）。

### **四、SVM的本质：几何直觉与核技巧的结合**

#### **1. 核心优势**  
  - **数据效率高**：仅依赖支持向量，不依赖全部数据，适合小样本学习。  
  - **抗过拟合能力强**：通过最大间隔和核函数的“局部敏感性”控制模型复杂度。  
  - **可解释性较好**：支持向量和核函数的选择有明确的几何/物理意义。

#### **2. 局限性**  
  - **计算复杂度高**：处理大规模数据（如百万级样本）时效率低，需结合近似算法。  
  - **参数调优依赖经验**：核函数和参数（如$C, \sigma$）的选择需要交叉验证，缺乏普适性。


### **五、一句话总结SVM**  
**支持向量机是一种通过最大化分类间隔实现“硬刚线性问题，巧解非线性问题”的算法**：  

- 线性可分时，找最宽的“安全线”；  
- 非线性可分时，用核函数把数据“搬到”高维空间，再找宽的“安全线”；  
- 有噪声时，适当放宽要求，在间隔和误差间找平衡。  

通过核函数的“魔法”，SVM用简单的数学工具解决了复杂的非线性分类问题，成为机器学习中的“瑞士军刀”之一。



## 第九章 深度学习

#### 为什么需要深度学习？

​	深度学习能用**更少的参数**和**更简单的结构**，实现复杂函数的表示。比如，一个隐藏层的神经网络可以拟合任意连续函数，但 “深度”（多层）网络比 “胖”（单层多神经元）网络更高效，就像用多层逻辑门比用大量单层逻辑门更简单。

### Softmax：排概率

- **作用**：把神经网络的输出（比如图像属于猫、狗、车的分数）转化为概率值，总和为 1，方便判断类别。
	**例子**：输入一张猫的图片，Softmax 输出 “猫 = 80%，狗 = 15%，车 = 5%”，选概率最高的类别作为预测结果。
- **本质**：通过指数运算**放大差异**，让大的分数更大、小的更小，突出主要类别，常用于分类任务的最后一层。



### CNN

#### CNN 的核心组件

- **卷积层**：模拟人类视觉对**局部特征的感知**，只关注图像中的小区域（如边缘、纹理），通过滑动窗口（卷积核）**提取特征**，且不同位置共享参数（**权值共享**），大幅减少计算量。例如，检测图像中的 “鸟喙” 时，同一个卷积核可以在不同位置重复使用。
- **池化层**：对特征图进行下采样（如取最大值或平均值），缩小尺寸，减少计算量，同时保留主要特征，类似压缩图片但保留关键信息。
- **全连接层**：将提取的特征整合，用于分类或回归，类似传统神经网络的输出层。



#### 卷积（CNN 核心）

- **核心思想**：用一个小窗口（卷积核）在图像上滑动，每次只关注局部区域（如边缘、纹理），提取重复出现的特征。
	**例子**：检测 “鸟喙” 时，同一个卷积核在图像不同位置滑动，只要出现类似形状就会激活。

	- 计算方法：对应相乘再相加

- 关键参数

	- **Stride（步长）**：窗口滑动的间隔，步长越大，输出特征图越小，计算越快但可能漏细节。

	- **Padding（填充）**：在图像边缘补零，避免边缘特征被忽略，保持输出尺寸不变。

		![image-20250527005235283](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250527005235283.png)

	- **多通道（彩色图）**：彩色图有 RGB 三个通道，卷积核也对应 3 个通道，分别提取特征后相加，处理彩色图时需匹配通道数。

		![image-20250527005219549](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250527005219549.png)

	- **偏置（Bias）**：给卷积结果加一个可学习的偏移量，增加模型灵活性，类似线性函数中的截距。

- **感受野**：神经元在原始图像中看到的区域大小，深层神经元通过多层卷积叠加，能捕捉更大范围的特征（如 “整只鸟” 由 “喙 + 身体 + 翅膀” 的局部特征组合而成）。

#### 池化（Pooling）：「特征压缩」，去掉冗余

- 作用：对卷积后的特征图进行**下采样**，缩小尺寸，减少计算量，同时保留关键特征。
	- **最大池化**：取窗口内最大值，保留最显著特征（如边缘的最强信号）。
	- **平均池化**：取平均值，平滑特征，减少噪声。
- **本质**：模拟人类视觉对细节的抽象能力，比如看人脸时，先关注眼睛、鼻子的位置（池化后的粗略位置），再看细节（卷积的局部特征）。

![image-20250527005320291](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250527005320291.png)

#### 归一化（标准化）：让数据「守规矩」，训练更稳定

- 作用：把每层输入数据的分布拉回到均值为 0、方差为 1 的范围，避免数据 “贫富差距过大” 导致训练困难（如梯度消失或爆炸）。
	- **Batch Normalization（BN）**：在训练时计算一个批次数据的均值和方差，测试时用全局统计量，加速收敛并允许更大的学习率。
- **类比**：考试后老师把全班分数 “标准化”，避免个别极高或极低分影响整体教学，让模型更容易学习到稳定的规律。

#### 残差网络（ResNet）：给深层网络「搭捷径」，解决训练难题

- **痛点**：传统深层网络因梯度消失 / 爆炸难以训练，精度可能随层数增加而下降。
- **创新**：引入 “残差连接”，让神经元可以直接学习 “当前层输出与输入的差异”，而不是从头学习完整特征。
	**公式**：输出 = 输入 + 残差函数（F (x)），相当于给信息开了一条 “绿色通道”，绕过复杂的多层变换。
- **效果**：轻松15训练 100 层以上的网络，精度显著提升，是现代深层网络的基础架构。

![image-20250527005537784](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250527005537784.png)



#### 目标检测：从「大海捞针」到「精准定位」

- **任务**：在图像中找出多个目标的位置（边框）和类别，比单纯分类更复杂（需同时解决 “在哪” 和 “是什么”）。
- **核心挑战**：目标大小、位置、姿态多变，背景复杂，需平衡速度与精度。

##### **关键指标**：

- **mAP（平均精度）**：综合所有类别，计算不同置信度下的查准率（“预测为猫的结果中真的是猫的比例”）和查全率（“所有真猫中被检测到的比例”），取平均值。
- **F1分数**：查准率和查全率的调和平均数，平衡两者的指标。

##### **经典网络**：

- 两阶段检测器

	（先找候选区域，再分类）：

	- **Faster R-CNN**：用 RPN 网络自动生成候选区域，共享特征计算，精度高但速度较慢（适合服务器端）。

- 单阶段检测器(直接预测）：

	- **YOLO**：将图像划分为网格，每个网格直接预测目标边框和类别，速度极快（实时 30 帧 / 秒），适合手机、摄像头等实时场景。
	- **SSD**：在不同尺度的特征图上预测目标，兼顾大小物体，精度与速度平衡。



### 循环神经网络


#### RNN 的核心特征  
1. **时序建模能力**  
   - 专为处理**序列数据**（如文本、语音）设计，通过隐状态（Hidden State）传递时序信息，捕捉上下文依赖关系。  
   - 例：在语言模型中，根据前文“我喜欢吃”预测后文“苹果”或“香蕉”。  

2. **权值共享机制**  
   - 所有时间步共享同一组参数矩阵（如 $W_{hh}, W_{xh}$），大幅减少参数数量，避免过拟合。  

3. **隐状态的记忆性**  
   - 隐状态 $h_t$ 融合当前输入 $x_t$ 和前一时刻隐状态 $h_{t-1}$，理论上可记忆长期信息，但实际受限于**梯度消失/爆炸**问题。  


#### RNN 的基本公式与作用  
**核心公式**（Vanilla RNN）：  
$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)  
$  
$
y_t = W_{hy} h_t + b_y  
$  
- **作用**：  
  - 通过循环连接实现**时序信息传递**，解决传统神经网络无法处理序列顺序的问题。  
  - 适用于**序列标注**（如词性标注）、**序列生成**（如文本续写）等任务。  
- **局限性**：长序列中梯度消失/爆炸问题显著，导致记忆能力下降。 

#### RNN结构图 

![image-20250527011600243](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250527011600243.png)


#### LSTM 的基本结构与关键术语  
**核心结构**：  
1. **记忆单元（Cell State）**：存储长期信息，通过门控机制控制更新。  
2. **三大门控单元**：  
   - **遗忘门（Forget Gate）**：决定丢弃历史信息（如句子中过时的代词指代）。  
   - **输入门（Input Gate）**：筛选当前输入的有效信息（如识别新出现的关键词）。  
   - **输出门（Output Gate）**：控制隐状态的输出内容（如过滤无关信息）。  

**特征与作用**：  

- **特征**：通过门控机制实现**自适应记忆**，缓解梯度消失问题，擅长捕捉长距离依赖。  
- **作用**：提升长序列（如长文本、语音信号）的建模能力，广泛应用于机器翻译、语音识别等领域。  

**结构图**

![image-20250527011738931](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250527011738931.png)

#### Self-Attention 的基本原理与关键术语

**核心机制**：  
- **Query-Key-Value（Q-K-V）**：  
  - **Query**：用于匹配序列中的其他元素（类似“查询条件”）。  
  - **Key**：作为被匹配的索引（类似“标签”）。  
  - **Value**：用于提取具体信息（类似“数据内容”）。  
- **注意力分数计算**：通过点积计算元素间的关联度，经Softmax生成权重，加权求和得到输出。  

**特征与作用**：  
- **特征**：  
  - **并行计算**：无需时序依赖，可同时处理序列中所有元素，适合长序列。  
  - **全局关联**：直接捕捉任意位置的依赖关系，解决RNN的长距离依赖缺陷。  
- **作用**：  
  - 提升模型对长序列的处理效率（如Transformer在NLP中的应用）。  
  - 避免RNN的顺序计算瓶颈，支持大规模并行训练。  


#### 对比总结  
| 模型       | 核心优势                          | 典型应用场景                |  
|------------|-----------------------------------|-----------------------------|  
| RNN        | 时序建模基础，参数效率高          | 短序列处理（如情感分析）    |  
| LSTM       | 长序列记忆能力强，抗梯度消失      | 机器翻译、语音识别          |  
| Self-Attention | 并行计算与全局关联，长序列高效   | Transformer架构、图像生成    |





## 第十一章 vision transfomer不看

## 十三章A 遗传算法(GA)

- 核心概念：
	- **遗传**：子代保留父代特征，维持物种稳定性。
	- **变异**：子代与父代的差异，是生物多样性的根源。
	- **自然选择**：适应环境的个体被保留，不适应者被淘汰，推动进化。
- 遗传学概念映射：
	- **染色体**：遗传物质载体，对应算法中的个体编码。
	- **基因**：染色体的基本单位，对应编码中的特征参数。
	- **适应度**：个体对环境的适应程度，对应算法中的目标函数值。

### 核心思路与特点

- 基本思路：
	- 通过**选择、交叉、变异**等遗传操作模拟生物进化过程，逐代优化种群，寻找最优解。
	- **流程**：初始种群→计算适应度→选择→交叉→变异→新一代种群，循环直至满足终止条件。
- 特点：
	- **自组织与自适应**：无需明确数学规则，通过进化过程自行优化。
	- **本质并行性**：可同时处理种群中的多个个体，适合大规模搜索。
	- **概率性搜索**：基于概率规则操作，避免陷入局部最优。
-  核心优势
	- **全局搜索能力**：通过种群多样性和概率操作，避免陷入局部最优。
	- **无需数学模型**：适用于目标函数复杂或未知的场景（如组合优化）
	- **搜索规模大**：并行性，可14同时处理多个个体，适合大规模搜索。
- 局限
	- **参数敏感性**：种群规模、交叉 / 变异概率需调优，否则影响收敛效率。
	- **计算复杂度**：高维问题中，种群规模大导致计算量激增。
	- **早熟收敛**：种群多样性不足时，可能过早锁定局部最优解。

### 基本遗传算法流

1. 编码与解码：

	- **编码**：将问题解映射为染色体（如二进制串、浮点数）。 *例*：求解函数优化问题时，用二进制串表示变量值。
	- **解码**：将染色体转换为实际解。 *例*：二进制串转换为区间内的实数。

2. 适应度函数设计：

	- 直接使用目标函数或其变形（如最大化问题取原值，最小化问题取负值）。
	- *要求*：非负、反映解的优劣，如函数优化中直接取目标函数值。

3. 遗传操作：

	- 选择：根据适应度概率选择个体，常见方法：
		- **轮盘赌选择法**：适应度越高，被选中概率越大（公式：$P(x_i) = \frac{f(x_i)}{\sum f(x_j)}$）。
		- **随机遍历抽样**：均匀间隔选择个体，避免轮盘赌的随机性偏差。
	- 交叉（基因重组）：
		- **单点交叉**：随机选择一个交叉点，交换两个父代的部分基因。
		- **多点交叉 / 均匀交叉**：多个交叉点或按概率随机交换基因。
	- 变异：
		- **二进制变异**：以低概率翻转二进制位（如 0→1 或 1→0）。
		- **实值变异**：对实数编码的个体添加随机扰动。
	- 迭代
		- 适应度低的淘汰

	

## 十三B 粒子群优化算法（PSO）学习笔记

### 一、发展历史
- **提出**：1995年由Kennedy和Eberhart提出，灵感源于鸟群捕食行为的研究。
- **特点**：简单易行、收敛速度快、设置参数少，是现代优化方法中广泛应用的群体迭代算法。

### 二、基本思想
- **核心模拟**：鸟群协作觅食行为。
  - 场景假设：鸟群在未知食物位置的区域觅食，每只鸟能感知当前位置与食物的距离（适应度），通过与其他鸟的信息交互（共享最优位置），调整飞行方向和速度，最终找到食物（最优解）。
  - **本质**：通过个体“经验”（自身曾找到的最优位置）和群体“协作”（群体当前找到的最优位置）的动态平衡，实现全局搜索。
- **群智能特征**
  - 个体能力简单，但通过信息交互，群体能力远超个体能力之和。
  - 典型例子：鸟群避障、蚁群路径优化等。

### 三、基本PSO算法
#### （一）核心概念
- **粒子**：每个解对应一只“鸟”，在D维空间中用位置 $X_i$ 和速度 $V_i$ 表示。
- **适应度**：通过适应度函数评估解的质量（如距离食物的远近）。
- **记忆**
  - **个体最优（pbest）**：粒子自身历史最佳位置。
  - **全局最优（gbest）**：群体中所有粒子的历史最佳位置。

#### （二）速度与位置更新公式
- **速度更新公式**：$v_{id}^k = w \cdot v_{id}^{k-1} + c_1r_1(pbest_{id} - x_{id}^{k-1}) + c_2r_2(gbest_d - x_{id}^{k-1})$
  - **三部分含义**
    - **惯性项（$w \cdot v_{id}^{k-1}$）**：粒子前一次的速度，体现“惯性”，维持运动趋势。
    - **认知项（$c_1r_1(\text{pbest} - \text{当前位置})$）**：粒子自身经验，向自己曾找到的最优位置学习。
    - **社会项（$c_2r_2(\text{gbest} - \text{当前位置})$）**：群体协作，向群体最优位置学习。
  - **参数作用**
    - $w$（惯性权重）：控制全局搜索（大w）与局部搜索（小w）的平衡。
    - $c_1, c_2$（学习因子）：控制个体经验与群体协作的权重，通常取2左右。
    - $r_1, r_2$（随机数）：增加搜索随机性，避免早熟。
- **位置更新公式**：$x_{id}^k = x_{id}^{k-1} + v_{id}^k$粒子根据新速度移动到新位置。

#### （三）算法流程
1. **初始化**：随机生成粒子的位置和速度。
2. **评估适应度**：计算每个粒子的适应度值。
3. **更新个体最优（pbest）**：比较当前适应度与粒子历史最优，更新pbest。
4. **更新全局最优（gbest）**：比较当前适应度与群体最优，更新gbest。
5. **更新速度和位置**：根据公式调整粒子的速度和位置。
6. **终止条件**：达到最大迭代次数或适应度值不再显著变化。

#### （四）关键要素
- **群体大小（m）**：m小易陷入局部最优，m大优化能力强但收敛慢，通常取20-100。
- **最大速度（$V_{max}$）**：限制粒子移动范围，避免“飞过”最优解，一般设为变量范围的10%-20%。
- **邻域拓扑结构**
  - **全局版（gbest）**：收敛快但易早熟。
  - **局部版（仅邻域最优）**：收敛慢但不易陷入局部最优。

### 四、示例解析（以四维Rosenbrock函数为例）
- **目标**：最小化 $f(x) = \sum_{i=1}^3 [100(x_{i+1}-x_i^2)^2 + (x_i-1)^2]$。
- **步骤**
  1. 初始化5个粒子的位置和速度（随机生成）。
  2. 计算初始适应度，确定pbest和gbest。
  3. 迭代更新速度和位置，逐步逼近最优解（理论最优解为所有$x_i=1$，$f(x)=0$）。
- **关键观察**：通过个体经验与群体协作，粒子逐渐向最优区域聚集。

### 五、算法改进
- **惯性权重优化**
  - **线性递减权值**：$w = w_{max} - (w_{max}-w_{min}) \cdot \frac{\text{当前迭代次数}}{\text{总迭代次数}}$初期用大w全局搜索，后期用小w局部精细调整。
  - **收缩因子法**：通过公式引入收缩因子K，强制算法收敛，提升解的质量。
- **混合策略**：结合遗传算法、模拟退火等，改善局部最优问题。

### 六、应用与挑战
- **应用领域**：系统设计、多目标优化、调度、机器人路径规划、信号处理等。
- **现存问题**
  - 理论基础薄弱：缺乏严格的收敛性证明，参数设置依赖经验。
  - 易陷入局部最优：尤其在高维复杂问题中。
- **研究热点**：收敛性分析、拓扑结构优化、参数自适应调整、混合算法设计。

### 七、与遗传算法（GA）的对比
| **对比项** | **PSO** | **遗传算法（GA）** |
|------------------|------------------------------|------------------------------|
| **核心机制** | 速度更新（惯性+经验+协作） | 交叉、变异、选择 |
| **记忆性** | 粒子保留个体和全局最优 | 种群进化，无显式记忆 |
| **参数复杂度** | 参数少（w, c1, c2等） | 参数多（交叉率、变异率等） |
| **收敛速度** | 通常更快 | 较慢 |
| **全局搜索能力** | 依赖权重调整 | 依赖遗传操作 |

### 总结
- **本质**：模拟群体协作的智能优化算法，通过个体经验与群体信息的动态交互，在解空间中搜索最优解。
- **优势**：简单高效、参数少、易实现，适合处理连续优化问题。
- **核心技巧**：合理调整惯性权重和学习因子，平衡全局探索与局部开发，避免陷入局部最优。







## 十三C 蚁群

### 一、生物基础：靠“化学信号”找最短路径

蚂蚁搬家看似漫无目的地乱走，却总能慢慢走出一条最优路径。这背后藏着两个关键机制：
1. **信息素：蚂蚁的隐形地图**  
   蚂蚁走过的地方会留下一种化学物质叫**信息素**，路径越短，信息素浓度越高。后续蚂蚁通过感知信息素浓度选择路线，形成“越多人走的路越容易被选择”的循环。  
   *举个例子*：  
   - 当两条路长度相同时，蚂蚁随机选择，但走的人多的路径会积累更多信息素，最终“垄断”所有蚂蚁的选择。  
   - 当一条路短一条路长时，短路上的信息素积累更快，蚂蚁会更快“跳槽”到短路上。  

2. **正反馈与探索的平衡**  
   - **正反馈**：信息素越积越多，让优质路径被更多蚂蚁选择，快速收敛到最优解（类似“从众心理”）。  
   - **随机探索**：少数蚂蚁会偶尔走新路，避免整个群体被困在局部最优（比如突然出现的障碍物，需要探索新路径）。

### 二、把生物行为翻译成数学模型

科学家把蚂蚁的行为抽象成一个**智能优化算法**，专门解决“找最优路径”类问题（比如旅行商问题TSP）。核心思想是：  
**用多只虚拟蚂蚁模拟真实蚂蚁，通过信息素的积累和挥发，让算法在搜索空间中逐步逼近最优解**。

#### 算法三要素：
1. **蚂蚁的“记忆”：禁忌表**  
   每只蚂蚁有一个禁忌表(Tabu)，记录自己走过的城市，确保不重复访问，形成合法路径。  

2. **选择路径的“规则”：概率公式**  
   蚂蚁选择下一个城市时，会同时考虑：  
   - **信息素浓度（τ）**：代表历史经验，浓度越高越容易被选。  
   - **能见度（η=1/距离）**：代表眼前的“直觉”，距离越近越容易被选。  
   两者通过一个公式算出选择概率：  
   $p_{ij}^k(t) = \frac{[\tau_{ij}(t)]^\alpha [\eta_{ij}(t)]^\beta}{\sum_{s\in 可选城市} [\tau_{is}(t)]^\alpha [\eta_{is}(t)]^\beta}$
   - $\alpha$：调大时更依赖历史经验（信息素），容易收敛但可能错过新路。  
   - $\beta$：调大时更依赖距离（直觉），探索性强但收敛慢。  

3. **信息素的“更新规则”：蒸发与积累**  
   - **蒸发**：每次迭代后，信息素会按比例（ρ）挥发（模拟自然挥发），避免旧信息“垄断”。  
   - **积累**：蚂蚁走完一圈后，按路径长度释放信息素，路径越短释放越多（奖励优质解）。  

#### 算法流程：

1. **初始化**：在所有城市间撒一点“初始信息素”（比如全设为1），放一批蚂蚁到随机城市。  
2. **蚂蚁周游**：每只蚂蚁按概率公式选下一个城市，用禁忌表避免重复，直到走完所有城市。  
3. **更新信息素**：所有蚂蚁走完后，旧信息素先挥发，再根据每只蚂蚁的路径长度，在其走过的路上增加新信息素。  
4. **重复直到收敛**：不断迭代，直到所有蚂蚁都走同一条路（最优解）或达到最大次数。

### 三、为什么它能解决复杂问题？三大核心优势
1. **分布式并行搜索**：多只蚂蚁同时探索不同路径，比单个“聪明算法”更快覆盖全局。  
2. **自适应调整**：信息素会根据搜索结果自动“优胜劣汰”，好路径的信息素越来越浓，差路径逐渐消失。  
3. **鲁棒性强**：不依赖问题的具体数学结构，稍微改改参数就能用于车间调度、网络路由等场景。

### 四、常见问题：算法的“软肋”与改进思路

- **问题1：容易“堵车”在非最优路径**  
  *原因*：如果某条路径信息素积累太快，蚂蚁会被困在局部最优（比如长路径因先走的蚂蚁多，信息素反而更浓）。  
  *解决*：  
  - 增加“精英策略”：给当前最优路径额外撒信息素，强化它的吸引力。  
  - 限制信息素范围：规定信息素浓度不能超过某个最大值或低于最小值，避免“一家独大”。  

- **问题2：初期搜索效率低**  
  *原因*：初始信息素均匀分布，蚂蚁像无头苍蝇乱撞。  
  *解决*：  
  - 用贪心算法“冷启动”：先人为给短路径加点信息素，引导蚂蚁更快找到方向。  
  - 动态调整参数：初期让$\beta$更大（依赖距离），后期让$\alpha$更大（依赖经验）。  

### 五、总结

**蚁群算法是一场模拟蚂蚁社会的“集体寻宝游戏”：用信息素记录经验，用概率选择平衡探索与收敛，让简单个体通过协作涌现出解决复杂问题的智能。**  

就像真实蚂蚁不需要大脑指挥，算法也不需要全局视野，仅凭局部交互和简单规则，就能在混乱中“煮”出最优解——这就是生物启发算法的魅力。





## 模糊计算



但由于这部分代码部分不在笔试范围，笔试只考计算题，故重点关注计算部分

### 模糊逻辑

#### 模糊集合表示法

##### 有限集表示

例：对于地铁某一车门处候车 “人数多” 这个模糊集合 $A$，其论域为 $U = \{0, 1, 2, \ldots, 10\}$，设各元素的隶属函数依次为 $\{0, 0, 0.1, 0.1, 0.3, 0.5, 0.8, 0.8, 0.9, 1, 1\}$。

当论域U为**有限集**`{x1,x2,…,xn}`时，通常有以下三种方式

- Zadeh表示法
	$A = \frac{\mu_A(x_1)}{x_1} + \frac{\mu_A(x_2)}{x_2} + \cdots + \frac{\mu_A(x_n)}{x_n}$

	$
	A = \frac{0}{0} + \frac{0}{1} + \frac{0.1}{2} + \frac{0.1}{3} + \frac{0.3}{4} + \frac{0.5}{5} + \frac{0.8}{6} + \frac{0.8}{7} + \frac{0.9}{8} + \frac{1}{9} + \frac{1}{10}$
	$= \frac{0.1}{2} + \frac{0.1}{3} + \frac{0.3}{4} + \frac{0.5}{5} + \frac{0.8}{6} + \frac{0.8}{7} + \frac{0.9}{8} + \frac{1}{9} + \frac{1}{10}$

​	隶属度为零的部分可不写入。

- 序偶表示法

​	$\mathrm{A=\{(x_1,\mu_A(x_1)),(x_2,\mu_A(x_2)),...,(x_N,\mu_A(x_N))\mid x\in U\}}$

- 向量表示法

​	$\mathrm{A=[\mu_A(x_1)~\mu_A(x_2)~...~\mu_A(x_N)]}$

##### 连续域表示

 当论域U为有限**连续域**时，Zadeh表示法为

$\mathrm{A=\int_U\frac{\mu_A(x)}{x}}$​

示例：若以排队时长（min）为论域，并设U=[0，10]，设S表示模糊集合“排队时间短”，L表示模糊集合“排队时间长”。

![image-20250528095511933](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528095511933.png)

#### 模糊截集

![image-20250528095743348](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528095743348.png)

例 求模糊集合
$A = \frac{0.5}{u_1} + \frac{0.6}{u_2} + \frac{1}{u_3} + \frac{0.7}{u_4} + \frac{0.3}{u_5}$的λ截集，λ∈[0, 1]。
解 取λ分别为1, 0.7, 0.3，于是有
$A_1 = \{u_3\}, A_{0.7} = \{u_3, u_4\}, A_{0.3} = \{u_1, u_2, u_3, u_4, u_5\}$

$A_1 = \frac{1}{u_3}, A_{0.7} = \frac{1}{u_3} + \frac{1}{u_4}, A_{0.3} = \frac{1}{u_1} + \frac{1}{u_2} + \frac{1}{u_3} + \frac{1}{u_4} + \frac{1}{u_5}$​



### 模糊集合

#### 模糊集合的关系与运算

##### 关系

- 并集（最大算子与和算子）

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\vee\mu_{\mathrm{B}}(\mathrm{x})=\max[\mu_{\mathrm{A}}(\mathrm{x}),\mu_{\mathrm{B}}(\mathrm{x})]$

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\vee\mu_{\mathrm{B}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})+\mu_{\mathrm{B}}(\mathrm{x})-\mu_{\mathrm{A}}(\mathrm{x})\mu_{\mathrm{B}}(\mathrm{x})$

- 交集（最小算子与乘积算子)

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\wedge\mu_{\mathrm{B}}(\mathrm{x})=\min[\mu_{\mathrm{A}}(\mathrm{x}),\mu_{\mathrm{B}}(\mathrm{x})]$

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\wedge\mu_{\mathrm{B}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\mu_{\mathrm{B}}(\mathrm{x})$

- 补集

	![image-20250528100732080](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528100732080.png)

	模糊集合**不满足**普通集合中的**补余律**，这表明在模糊集合中存在其补集等于自己的集合。

##### 运算性质

![image-20250528100815272](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528100815272.png)



##### 集合直积

![image-20250528101104091](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528101104091.png)



#### 模糊集合关系

##### 定义

模糊关系是定义在直积空间上的模糊集合，所以它也遵从一般模糊集合的运算规则。

![image-20250528101428637](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528101428637.png)



##### 模糊集合的合成

![image-20250528101801354](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528101801354.png)

**最大——最小合成规则**

![image-20250528115324688](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528115324688.png)





### 模糊聚类

闭包和模糊聚类应该属于代码内容，不管了

### 模糊推理



#### 简单模糊推理表示

- 模糊蕴含最小运算(**Mamdani**)

	$R_c=A\to B=A\times B=\int_{X\times Y}\mu_A(x)\wedge\mu_B(y)/(x,y)$

- 模糊蕴含算术运算(Zadeh)
- 模糊蕴含的最大最小运算(Zadeh)

例：

![image-20250528102755457](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528102755457.png)

#### 简单模糊条件语句

![image-20250528103037287](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528103037287.png)

#### 多重模糊条件语句

![image-20250528104228597](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528104228597.png)

![image-20250528104315679](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528104315679.png)

![image-20250528104401748](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528104401748.png)

![image-20250528104909029](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528104909029.png)

#### 单点模糊化

![image-20250528105753889](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528105753889.png)

#### 清晰化计算

通过模糊推理得到的是模糊量，而对于实际的控制则必须为清晰量，因此需要将模糊量转换成清晰量，

- 平均最大隶属度法(mom)

	![image-20250528105948805](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528105948805.png)

- 加权平均法(面积重心法centroid)

	![image-20250528110026429](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110026429.png)

	##### 离散示例

	1. 平均最大隶属度法(mom)

	![image-20250528110241797](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110241797.png)

	2. **加权平均法(面积重心法centroid)**

	![image-20250528110417191](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110417191.png)

	

	##### 连续示例

	1. 平均最大隶属度法

		![image-20250528110609058](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110609058.png)

	2. 加权平均法

		![image-20250528110621211](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110621211.png)

	

	3. 最大隶属度取最小值方法(som)取模糊集合中具有最大隶属度的所有点中的最小的一个作为去模糊化的结果。
	4. 最大隶属度取最大值方法(lom)取模糊集合中具有最大隶属度的所有点中的最大的一个作为去模糊化的结果。
	5. 中位数法(面积平分法bisector)中位数法是取隶属度的中位数作为z的清晰量

	

	#### 模糊系统实验

	matlab实现见实验报告

	

	## 模糊计算总结和例题

	1. 并(取最大)

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\vee\mu_{\mathrm{B}}(\mathrm{x})=\max[\mu_{\mathrm{A}}(\mathrm{x}),\mu_{\mathrm{B}}(\mathrm{x})]$

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\vee\mu_{\mathrm{B}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})+\mu_{\mathrm{B}}(\mathrm{x})-\mu_{\mathrm{A}}(\mathrm{x})\mu_{\mathrm{B}}(\mathrm{x})$

	

	2. 交（取最小）

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\wedge\mu_{\mathrm{B}}(\mathrm{x})=\min[\mu_{\mathrm{A}}(\mathrm{x}),\mu_{\mathrm{B}}(\mathrm{x})]$

	$\mu_{\mathrm{C}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\wedge\mu_{\mathrm{B}}(\mathrm{x})=\mu_{\mathrm{A}}(\mathrm{x})\mu_{\mathrm{B}}(\mathrm{x})$

	

	3. 补集（相减）

	![image-20250528100732080](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528100732080.png)

	​	模糊集合**不满足**普通集合中的**补余律**，这表明在模糊集合中存在其补集等于自己的集合。

	

	4. 点乘（合成）$\circ$​

		![image-20250528115324688](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528115324688.png)

		

	5. 叉乘（直积、模糊蕴含最小运算）$\times$​

		1. 直积

		![image-20250528101104091](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528101104091.png)

		2. 模糊蕴含最小运算

			$R_c=A\to B=A\times B=\int_{X\times Y}\mu_A(x)\wedge\mu_B(y)/(x,y)$​

			![image-20250528102755457](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528102755457.png)

	
	6. 清晰化计算​
		**算论域的值**
	
		1. 平均最大隶属度法(mom)
		
		**找隶属度最大的域值取平均**
		 离散
		
		![image-20250528110241797](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110241797.png)
			
		连续
			
		![image-20250528110609058](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110609058.png)
		
		2. 加权平均法
		![image-20250528110026429](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110026429.png)
			离散
		![image-20250528110417191](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110417191.png)
		
			连续
		![image-20250528110621211](./%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80.assets/image-20250528110621211.png)
	
	
	
	
	
	
	
	
	
	
	
	
